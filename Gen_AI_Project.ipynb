{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Neccessary Libraries"
      ],
      "metadata": {
        "id": "ltfnpjf_UqYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain langchain_community transformers langchain-openai \\ langchain-huggingface streamlit pyngrok unstructured tiktoken huggingface"
      ],
      "metadata": {
        "id": "Fkn1VkDnUqMm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Link Sage"
      ],
      "metadata": {
        "id": "1AIhW8Kn8Ihc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile linksage.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import time\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from transformers import pipeline\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import FAISS\n",
        "import textwrap\n",
        "\n",
        "st.title('LinkSage')\n",
        "st.sidebar.title('URLs')\n",
        "\n",
        "urls = []\n",
        "\n",
        "for i in range(3):\n",
        "  url = st.sidebar.text_input(f'URL {i+1}')\n",
        "  urls.append(url)\n",
        "\n",
        "process_url_clicked = st.sidebar.button('Process URLs')\n",
        "\n",
        "file_path = 'faiss_store_openai.pkl'\n",
        "\n",
        "main_placeholder = st.empty()\n",
        "\n",
        "### Using Langchain models\n",
        "\n",
        "repo_id = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = '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'\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id = repo_id, temperature = 0.5, token = HUGGINGFACEHUB_API_TOKEN,\n",
        ")\n",
        "\n",
        "### Using Huggingface Pipeline\n",
        "\n",
        "## llm = pipeline(\"question-answering\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "\n",
        "## llm = HuggingFacePipeline(pipeline=llm)\n",
        "\n",
        "if process_url_clicked:\n",
        "  # load data\n",
        "  loader = UnstructuredURLLoader(urls = urls)\n",
        "  main_placeholder.text('Data Loading... Started...')\n",
        "  data = loader.load()\n",
        "  # split data\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      separators = ['\\n\\n', '\\n', '.', ','],\n",
        "      chunk_size = 1000\n",
        "  )\n",
        "  main_placeholder.text('Text Splitter... Started...')\n",
        "  docs = text_splitter.split_documents(data)\n",
        "  # create embeddings and save it to FAISS index\n",
        "  embeddings = SentenceTransformerEmbeddings(model_name = 'all-MiniLM-L6-v2')\n",
        "  vectorstore_openai = FAISS.from_documents(docs, embeddings)\n",
        "  main_placeholder.text('Embedding Vector Started Building...')\n",
        "  time.sleep(1)\n",
        "\n",
        "  # Save the FAISS index to a pickle file\n",
        "  with open(file_path, 'wb') as f:\n",
        "    pickle.dump(vectorstore_openai, f)\n",
        "\n",
        "query = main_placeholder.text_input('Question: ')\n",
        "if query:\n",
        "  if os.path.exists(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "      vectorstore = pickle.load(f)\n",
        "      chain = RetrievalQAWithSourcesChain.from_llm(llm = llm, retriever = vectorstore.as_retriever())\n",
        "      result = chain({'question': query}, return_only_outputs = True)\n",
        "      # result will be a dictionary of this format --> {\"answer\": \"\", \"sources\": [] }\n",
        "\n",
        "      st.header('Answer')\n",
        "      st.write(result['answer'])\n",
        "\n",
        "      # display sources, if available\n",
        "      sources = result.get('sources', '')\n",
        "      if sources:\n",
        "        st.subheader('Sources:')\n",
        "        sources_list = sources.split('\\n') # split by the source to newline\n",
        "        for source in sources_list:\n",
        "          st.write(source)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UneOiCF1put",
        "outputId": "1d8f7260-880d-4c75-dad8-c6c13de0f3e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing linksage.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set authentic token if you haven't already done so\n",
        "ngrok.set_auth_token('32KRjcveObHEqeMweczQpyXdsmh_437s2vAbcoUTnfJoQuNCW')\n",
        "\n",
        "# Start Streamlit server on a specfic port\n",
        "!nohup streamlit run linksage.py --server.port 5011 &\n",
        "\n",
        "# Start ngrok tunnel to expose the Streamlit server\n",
        "ngrok_tunnel = ngrok.connect(addr = '5011', proto = 'http', bind_tls = True)\n",
        "\n",
        "# Print the URL of the ngrok tunnel\n",
        "print(' * Tunnel URL:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh4K7LXm60gh",
        "outputId": "5f7b7a1a-921b-4fb3-b1c5-71fb722f57d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            " * Tunnel URL: https://9d4a7f8a831b.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In free ngrok plan we can only run one ngrok server at a time so we have to kill other existing sites.\n",
        "!kill -9 $(pgrep ngrok) || echo"
      ],
      "metadata": {
        "id": "upKfvRqb02uC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blog Generation"
      ],
      "metadata": {
        "id": "WENjjf4Z8OTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile blog-gen.py\n",
        "import streamlit as st\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import CTransformers\n",
        "import os\n",
        "from langchain_huggingface.llms import HuggingFaceEndpoint\n",
        "\n",
        "## Function to get response from Llama 2 model\n",
        "\n",
        "def getLLamaresponse(input_text, no_words, blog_style):\n",
        "\n",
        "  ## Llama2 Model\n",
        "  repo_id = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "\n",
        "  os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
        "  HUGGINGFACEHUB_API_TOKEN = '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'\n",
        "\n",
        "  llm = HuggingFaceEndpoint(\n",
        "      repo_id = repo_id, temperature = 0.5, token = HUGGINGFACEHUB_API_TOKEN,\n",
        "  )\n",
        "\n",
        "  template = \"\"\"\n",
        "    Write a Blog for {blog_style} job profile for a topic {input_text}\n",
        "    within {no_words} words.\n",
        "      \"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(input_variable = ['blog_style', 'input_text', 'no_words'], template = template)\n",
        "\n",
        "  ## Generate the response from the Llama 2 Model\n",
        "\n",
        "  response = llm(prompt.format(blog_style = blog_style, input_text = input_text, no_words = no_words))\n",
        "  print(response)\n",
        "  return response\n",
        "\n",
        "\n",
        "st.set_page_config(page_title = 'Generate Blogs',\n",
        "                   page_icon = 'ðŸ¤–',\n",
        "                   layout = 'centered',\n",
        "                   initial_sidebar_state = 'collapsed')\n",
        "\n",
        "st.header('Blogsmith ðŸ¤–')\n",
        "\n",
        "input_text = st.text_input('Enter the Blog Topic')\n",
        "\n",
        "## creating 2 columns for additional 2 more fields\n",
        "\n",
        "col1, col2 = st.columns([5, 5])\n",
        "\n",
        "with col1:\n",
        "  no_words = st.text_input('No. of words')\n",
        "\n",
        "with col2:\n",
        "  blog_style = st.selectbox('Writing the blog for',\n",
        "                            ('Researchers', 'Data Scientist', 'Common People'), index = 0)\n",
        "\n",
        "submit = st.button('Generate')\n",
        "\n",
        "\n",
        "## Final Response\n",
        "\n",
        "if submit:\n",
        "  st.write(getLLamaresponse(input_text, no_words, blog_style))"
      ],
      "metadata": {
        "id": "OkTFzOBR7s_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252467b7-44fd-48d2-c99e-f4e49a0ed587"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing blog-gen.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set authentication token if you haven't already done so\n",
        "ngrok.set_auth_token('32KRjcveObHEqeMweczQpyXdsmh_437s2vAbcoUTnfJoQuNCW')\n",
        "\n",
        "# Start streamlit server on specfic port\n",
        "!nohup streamlit run blog-gen.py --server.port 5012 &\n",
        "\n",
        "# Start ngrok tunnel to expose the Streamlit server\n",
        "ngrok_tunnel = ngrok.connect(addr = '5012', proto = 'http', bind_tls = True)\n",
        "\n",
        "# Print the URL of the ngrok tunnel\n",
        "print(' * Tunnel URL:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQGSRgjgT8Qj",
        "outputId": "605eda28-10ee-42d6-ee9c-39c899027acd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            " * Tunnel URL: https://81183848fd5a.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In free ngrok plan we can only run one ngrok server at a time so we have to kill other existing sites.\n",
        "!kill -9 $(pgrep ngrok) || echo"
      ],
      "metadata": {
        "id": "tGs2m9gjYMF8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gen Outfit"
      ],
      "metadata": {
        "id": "cVFAsxIxyREn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile st.py\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from transformers import pipeline, set_seed\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "def generate_image(prompt):\n",
        "\n",
        "  pipe = DiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-xl-base-1.0', torch_dtype = torch.float16, use_safetensors = True, variant = 'fp16')\n",
        "  pipe.to('cuda')\n",
        "\n",
        "  # if using torch < 2.0\n",
        "  # pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "  image = pipe(prompt = prompt).images[0]\n",
        "  return image\n",
        "\n",
        "def main():\n",
        "\n",
        "  st.title('Generate Outfits using AI')\n",
        "  text_input = st.text_input('Enter Some Text ðŸ‘‡')\n",
        "\n",
        "  if st.button('Predict'):\n",
        "    x = generate_image(text_input)\n",
        "    st.image(x)\n",
        "\n",
        "  if __name__ =='__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMpWNn50YvT8",
        "outputId": "236bb3d1-2afe-45db-e9f2-a79b1921cb18"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing st.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set Authentication token if you haven't already done so\n",
        "ngrok.set_auth_token('32KRjcveObHEqeMweczQpyXdsmh_437s2vAbcoUTnfJoQuNCW')\n",
        "\n",
        "# Start Streamlit server on a specific port\n",
        "!nohup streamlit run st.py --server.port 5013 &\n",
        "\n",
        "# Start ngrok tunnel to expose the streamlit server\n",
        "ngrok_tunnel = ngrok.connect(addr = '5013', proto = 'http', bind_tls = True)\n",
        "\n",
        "# Print the Url of ngrok tunnel\n",
        "print(' * Tunnel URL:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6ObmKjmz-2F",
        "outputId": "99c3ba2d-210c-42eb-9733-9f575d601cb2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            " * Tunnel URL: https://e529b7be82e1.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In free ngrok plan we can only run one ngrok server at a time so we have to kill other existing sites.\n",
        "!kill -9 $(pgrep ngrok) || echo"
      ],
      "metadata": {
        "id": "sHUSypRw1AG2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boAFVj2GKpc-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}